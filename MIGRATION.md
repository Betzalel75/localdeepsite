# Migration Guide - LocalSite Hybrid Mode

Ce guide vous aide √† migrer votre installation LocalSite existante vers le nouveau syst√®me hybride qui supporte √† la fois les mod√®les locaux et cloud.

## Vue d'ensemble des changements

### Nouveaut√©s
- ‚ú® Support des APIs cloud (DeepSeek, Gemini, OpenAI, Claude, etc.)
- ‚ú® Interface unifi√©e pour tous les mod√®les
- ‚ú® Auto-d√©tection des mod√®les et providers disponibles
- ‚ú® Validation automatique des cl√©s API
- ‚ú® Mode hybride (local + cloud)
- ‚ú® Composant de statut des providers en temps r√©el

### Compatibilit√©
- ‚úÖ 100% compatible avec les installations Ollama existantes
- ‚úÖ Vos mod√®les locaux continuent de fonctionner
- ‚úÖ Aucune modification requise pour une utilisation locale uniquement

## Migration par √©tapes

### √âtape 1 : Sauvegarde (optionnel mais recommand√©)
```bash
# Sauvegardez votre configuration actuelle
cp .env.local .env.local.backup
```

### √âtape 2 : Mise √† jour des variables d'environnement

#### Option A : Mode local uniquement (aucun changement requis)
Si vous voulez continuer √† utiliser uniquement les mod√®les locaux, votre configuration actuelle fonctionne toujours :

```env
LOCAL_MODE=true
NEXT_PUBLIC_LOCAL_MODE=true
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=deepseek-r1:7b
```

#### Option B : Ajout du support cloud
Pour ajouter des mod√®les cloud √† votre installation locale existante :

```env
# Gardez votre configuration locale existante
LOCAL_MODE=true
NEXT_PUBLIC_LOCAL_MODE=true
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=deepseek-r1:7b

# Activez le mode hybride
ENABLE_MIXED_MODE=true
NEXT_PUBLIC_ENABLE_MIXED_MODE=true

# Ajoutez vos cl√©s API (optionnel)
DEEPSEEK_API_KEY=your_deepseek_api_key
GEMINI_API_KEY=your_GEMINI_API_KEY
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key
GROQ_API_KEY=your_groq_api_key
```

#### Option C : Migration vers le cloud uniquement
Pour utiliser uniquement les mod√®les cloud :

```env
# D√©sactivez le mode local
LOCAL_MODE=false
NEXT_PUBLIC_LOCAL_MODE=false

# Ajoutez vos cl√©s API
DEEPSEEK_API_KEY=your_deepseek_api_key
GEMINI_API_KEY=your_GEMINI_API_KEY
# ... autres cl√©s API
```

### √âtape 3 : Red√©marrer l'application
```bash
npm run dev
```

### √âtape 4 : V√©rification
1. Ouvrez l'application dans votre navigateur
2. Allez dans les param√®tres (ic√¥ne d'engrenage)
3. V√©rifiez que vos mod√®les sont bien d√©tect√©s
4. Testez la g√©n√©ration avec diff√©rents mod√®les

## Configuration des cl√©s API

### DeepSeek (Recommand√© - Tr√®s abordable)
1. Cr√©ez un compte sur [platform.deepseek.com](https://platform.deepseek.com)
2. G√©n√©rez une cl√© API
3. Ajoutez dans `.env.local` :
```env
DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

### Google Gemini (Gratuit avec quotas g√©n√©reux)
1. Allez sur [aistudio.google.com](https://aistudio.google.com)
2. Cr√©ez une cl√© API
3. Ajoutez dans `.env.local` :
```env
GEMINI_API_KEY=AIzaSyxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

### OpenAI
1. Cr√©ez un compte sur [platform.openai.com](https://platform.openai.com)
2. G√©n√©rez une cl√© API
3. Ajoutez dans `.env.local` :
```env
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

### Anthropic Claude
1. Cr√©ez un compte sur [console.anthropic.com](https://console.anthropic.com)
2. G√©n√©rez une cl√© API
3. Ajoutez dans `.env.local` :
```env
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

### Groq (Tr√®s rapide, gratuit avec limites)
1. Cr√©ez un compte sur [console.groq.com](https://console.groq.com)
2. G√©n√©rez une cl√© API
3. Ajoutez dans `.env.local` :
```env
GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

## Fonctionnalit√©s de l'interface

### Nouveau composant de statut
- Indicateur en temps r√©el de l'√©tat des providers
- Accessible via l'ic√¥ne dans le header de l'√©diteur
- Affiche le nombre de mod√®les disponibles
- Permet de rafra√Æchir la d√©tection

### Param√®tres am√©lior√©s
- Filtrage par type de mod√®le (Local/Cloud/Tous)
- Auto-s√©lection du meilleur provider
- Validation des cl√©s API en temps r√©el
- Interface adaptative selon les providers disponibles

### S√©lection intelligente
- L'application recommande automatiquement le meilleur mod√®le disponible
- Priorit√© donn√©e aux mod√®les locaux si disponibles
- Fallback automatique en cas d'indisponibilit√©

## R√©solution de probl√®mes

### Les mod√®les cloud ne s'affichent pas
1. V√©rifiez que vos cl√©s API sont correctes dans `.env.local`
2. Red√©marrez l'application
3. V√©rifiez la console pour les erreurs de validation
4. Testez vos cl√©s avec `/api/check-api-keys`

### Les mod√®les locaux ont disparu
1. Assurez-vous qu'Ollama fonctionne : `ollama list`
2. V√©rifiez que `OLLAMA_BASE_URL` est correct
3. Red√©marrez Ollama : `ollama serve`
4. Rafra√Æchissez la d√©tection dans l'interface

### Erreurs de r√©seau
1. V√©rifiez votre connexion Internet
2. Certains providers peuvent √™tre bloqu√©s dans votre r√©gion
3. Utilisez un VPN si n√©cessaire
4. Consultez le statut des services des providers

### Performance d√©grad√©e
1. Les mod√®les cloud sont g√©n√©ralement plus rapides
2. V√©rifiez votre quota/limite de taux
3. Consid√©rez utiliser Groq pour la vitesse
4. Revenez aux mod√®les locaux si n√©cessaire

## Migration des projets existants

### Projets cr√©√©s avec des mod√®les locaux
- Continueront de fonctionner normalement
- Peuvent √™tre √©dit√©s avec des mod√®les cloud
- Pas de conversion n√©cessaire

### Historique des prompts
- Pr√©serv√© lors de la migration
- Compatible avec tous types de mod√®les
- Accessible depuis l'interface histoire

## Recommandations

### Pour un usage occasionnel
- Utilisez Google Gemini (gratuit avec quotas g√©n√©reux)
- Ou Groq (tr√®s rapide, gratuit avec limites)

### Pour un usage intensif
- DeepSeek (tr√®s abordable, excellente qualit√©)
- Combin√© avec des mod√®les locaux pour la confidentialit√©

### Pour la confidentialit√© maximale
- Gardez uniquement les mod√®les locaux
- Utilisez le mode `LOCAL_MODE=true`
- Aucune donn√©e ne sera envoy√©e sur Internet

### Configuration optimale
```env
# Mode hybride optimal
ENABLE_MIXED_MODE=true
NEXT_PUBLIC_ENABLE_MIXED_MODE=true

# Local pour la confidentialit√©
OLLAMA_BASE_URL=http://localhost:11434

# Cloud pour la performance
DEEPSEEK_API_KEY=your_key
GEMINI_API_KEY=your_key
GROQ_API_KEY=your_key
```

## Support et aide

### Logs de d√©bogage
- Ouvrez la console d√©veloppeur (F12)
- V√©rifiez l'onglet "Network" pour les erreurs API
- Consultez l'onglet "Console" pour les messages d'erreur

### Test de connectivit√©
Utilisez ces endpoints pour tester votre configuration :
- `/api/check-api-keys` - V√©rifie les cl√©s disponibles
- `/api/ollama-models` - Liste les mod√®les Ollama
- `/api/test-api-key` - Teste une cl√© API sp√©cifique

### Restauration en cas de probl√®me
```bash
# Restaurez votre configuration pr√©c√©dente
cp .env.local.backup .env.local
npm run dev
```

## Questions fr√©quentes

### Q: Mes mod√®les locaux sont-ils toujours prioritaires ?
R: Oui, si vous avez des mod√®les locaux disponibles, ils seront propos√©s en premier dans l'interface.

### Q: Mes donn√©es sont-elles envoy√©es au cloud sans mon consentement ?
R: Non, vous choisissez explicitement quel mod√®le utiliser. Les mod√®les locaux restent 100% priv√©s.

### Q: Puis-je m√©langer mod√®les locaux et cloud dans le m√™me projet ?
R: Oui, vous pouvez utiliser diff√©rents mod√®les pour diff√©rentes parties du projet.

### Q: Comment savoir si j'utilise un mod√®le local ou cloud ?
R: L'interface indique clairement le type de mod√®le (Local/Cloud) dans les param√®tres.

### Q: Que se passe-t-il si ma connexion Internet tombe ?
R: Vous pouvez continuer √† utiliser les mod√®les locaux normalement.

---

üéâ **F√©licitations !** Vous avez maintenant acc√®s au meilleur des deux mondes : la confidentialit√© des mod√®les locaux ET la puissance des mod√®les cloud !